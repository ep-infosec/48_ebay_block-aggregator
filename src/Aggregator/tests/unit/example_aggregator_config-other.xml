<config>
  <supportedTables>
    <supportedTable>
      <database>default</database>
      <table>simple_event</table>
      <insert_query>insert into simple_event (`Host`, `Colo`, `EventName`, `Count`, `Duration`, `TimeStamp`) VALUES ('graphdb-21', 'LVS', 'TERMINATED', 1,  1000,  '2019-06-15 23:00:06')</insert_query>
      <schema>create table simple_event (Host String, Colo String, EventName String, Count UInt64, Duration Float32, TimeStamp  DateTime)</schema>
      <shard>6000</shard>
      <replica>2000</replica>
    </supportedTable>

    <supportedTable>
      <database>default</database>
      <table>simple_event_3</table>
      <insert_query>insert into simple_event_3 (`Host`, `Count`) VALUES ('graphdb-36', 9900 )</insert_query>
      <schema>create table simple_event_3 (Host String, Count UInt64) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/simple_event_3', '{replica}') ORDER BY(Host, Count) SETTINGS index_granularity=8192</schema>
      <shard>6000</shard>
      <replica>2000</replica>
    </supportedTable>

     <supportedTable>
      <database>default</database>
      <table>simple_event_5</table>
      <insert_query>insert into simple_event_5 (`Count`, `Host`, `Colo`) VALUES (9901,'graphdb-36','SLC')</insert_query>
      <schema>create table simple_event_5 (Count UInt64, Host String, Colo String) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/simple_event_5', '{replica}') ORDER BY(Host, Count) SETTINGS index_granularity=8192</schema>
      <shard>6000</shard>
      <replica>2000</replica>
    </supportedTable>

  </supportedTables>


 <zookeeper>
    <node>
        <host>10.169.98.238</host>
        <port>2181</port>
    </node>
    <node>
        <host>10.169.106.125</host>
        <port>2181</port>
    </node>

    <node>
        <host>10.169.100.133</host>
        <port>2181</port>
    </node>

    <session_timeout_ms>30000</session_timeout_ms>
    <operation_timeout_ms>10000</operation_timeout_ms>

    <!-- Optional. Chroot suffix. Should exist. -->
    <!--<root>/path/to/zookeeper/node</root> -->
    <!-- Optional. Zookeeper digest ACL string. -->
    <!--<identity>user:password</identity> -->
 </zookeeper>

 <!--following the parameters defined in peer database server ConnectioParameters -->
 <!--with graphdb-5: 10.169.152.119 -->
 <peerDatabaseServer>
   <host>10.169.152.119</host>
   <is_secure>false</is_secure>
   <port>9000</port>
   <database>default</database>
   <user>default</user>
   <password></password>
   <compression>true</compression>

   <!--
   <connection_timeout>30000</connection_time_out>
   <send_timeout>30000</send_timeout>
   <receive_timeout>30000</receive_timeout>
   <tcp_keep_alive_timeout>30000 </tcp_keep_alive_timeout>
   <http_keep_alive_timeout>30000 </http_keep_alive_timeout>
   -->
   
 </peerDatabaseServer>

 <KafkaConnections>
     <KafkaConnection>
        <zone>lvs</zone>
        <topic>my-test-topic</topic>
        <group_id>my-consumer-group-id</group_id>

        <!-- list of the server and the ports associated with the bootstrap servers--> 
        <servers>
            <server>10.156.17.36:9092</server>
            <server>10.156.27.134:9092</server>
            <server>10.196.78.142:9092</server>
        </servers>

        <!-- default to be: false --> 
        <enable_auto_commit>false</enable_auto_commit>
        <!-- default to be false -->
        <enable_auto_offset_store>false</enable_auto_offset_store>
        <!-- choose from "earliest" or "latest"-->
        <auto_offset_reset>earliest</auto_offset_reset>

        <!-- batch size in bytes -->
        <kafka_batch_size>1000000</kafka_batch_size>
        <!-- batch timeout in ms -->
        <kafka_batch_timeout>1000</kafka_batch_timeout>

        <!-- accumulated block processing size in bytes -->
        <buffer_batch_processing_size>1000000</buffer_batch_processing_size>
        <!-- accumulated block processing size in ms -->
        <buffer_batch_processing_timeout>1000</buffer_batch_processing_timeout>
    </KafkaConnection>

 </KafkaConnections>   
</config> 
